{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4cbgwZWWfWpp"
   },
   "source": [
    "# New Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7c7NqCvw8gh"
   },
   "source": [
    "#Introduction: \n",
    "At ****, we continuously strive to give better song recommendations to our users. As a part of this assignment, you will have to read the research paper, **[Long and Short-Term Recommendations with Recurrent Neural Networks](http://iridia.ulb.ac.be/~rdevooght/papers/UMAP__Long_and_short_term_with_RNN.pdf)**, and implement a recurrent neural network based collaborative filtering recommendation system. \n",
    "\n",
    "Collaborative filtering utilizes the user-item interactions to recommend relevant items to a user (based on the interaction of similar users). RNNs are part of growing interest for collaborative filtering based on sequence prediction. This paper shows how recurrent neural networks can be steered towards better short and long-term predictions.\n",
    "\n",
    "The aim of this assignment is to reproduce **the SPS (short term prediction score) metric** on the test set, which is mentioned in **Table 3**. Please note that only the method **RNN-CCE** is expected to be reproduced and that too only on the **Movielens** dataset.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#Dataset: \n",
    "**Movielens 1 Million data**\n",
    "- 1 Million user-movie interactions\n",
    "- 6040 users\n",
    "- 3706 unique movies\n",
    "\n",
    "You are being provided with the preprocessed dataset. \n",
    "\n",
    "Each row of training data is an array. The first element of the array is a unique `user_id`, the subsequent elements are `movie_id` and `rating` of the user for that movie, which then repeats.\n",
    "\n",
    "`i.e. user_id movie_id1 rating1 movie_id2 rating2 …`\n",
    "\n",
    "For each user, the array contains movies in the order of timestamp in which he/she has rated the movies. \n",
    "\n",
    "It is worth mentioning that you do not need to use the values of the ratings in any way, you are only supposed to predict which movie a user will rate, based on what he rated before. But if you can find any way to use it, to improve the metric, all innovations will be welcomed.\n",
    "\n",
    "**Training Data:** Movies interaction of 5040 users are provided for training purposes.\n",
    "\n",
    "**Validation Data:** Movies interaction of 500 users are provided for validation purposes.\n",
    "\n",
    "**Test Data:** Movies interaction of 500 users are provided for testing purposes.\n",
    "\n",
    "---\n",
    "\n",
    "# Submission Guidelines: \n",
    "\n",
    "1. You can use any programming language and Deep Learning framework of your choice for submission (preferred Keras).\n",
    "2. Training Step: Create and train a model that matches the SPS metric that is mentioned in the paper. Also, the final model (or just model weights) should be saved so that it could be used for evaluating the metric later. The saved model (or model weights) should be named ‘best_model’ followed by a suitable extension, depending on the Deep Learning framework in use.\n",
    "3. Model Loading Step: Load the saved model (or model weights). \n",
    "4. Prediction Step: Run the loaded model and evaluate it on the **test data**, printing the SPS metric. \n",
    "5. If you are using the GPU provided by colab, and your implementaton is taking way beyond 1 hour for training, then you should be worried about your implementation.\n",
    "6. Please ensure that the submitted code is well-commented and structured for easy comprehension.\n",
    "7. Please put your colab notebook and saved model (or model weights) file in a zipped file named after you. Also, share the colab link along with the above zipped file over mail.\n",
    "8. Please write the answers to the questions aksed at the bottom of this notebook. \n",
    "\n",
    "---\n",
    "\n",
    "# Evaluation Guidelines:\n",
    "\n",
    "1. You are required to run your model on the test set of 500 users and print the SPS metric. The evaluation will be based on the reproducibility of the SPS metric as mentioned in the research paper. \n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "# NOTE:\n",
    "The code provided in the paper is not very easy to comprehend, and it is difficult to reproduce the results (mentioned in tha paper), using this code. We found it much easier to write our own code from scratch, to reproduce the results mentioned in the paper.  \n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "We will be helping you with the preprocessed data. You just need to run the following few cells to download and unzip it. Once you have the data ready, you can take it on from there.  \n",
    "\n",
    "# We wish you all the best! 😁\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "# Download and Unzip Data\n",
    "\n",
    "The dataset for training and testing the model has been uploaded to s3. Run the following cell to download the zipped data.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gM_f6tscrUs7",
    "outputId": "5443c115-aa00-47db-9607-ee5bcb22defd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-05-16 00:51:29--  https://drive.google.com/uc?export=download&id=1xUB9_bMs-PYnRbO7eedXFh0qS6P4bA-X\n",
      "Resolving drive.google.com (drive.google.com)... 142.250.148.102, 142.250.148.138, 142.250.148.139, ...\n",
      "Connecting to drive.google.com (drive.google.com)|142.250.148.102|:443... connected.\n",
      "HTTP request sent, awaiting response... 303 See Other\n",
      "Location: https://doc-14-3s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/ksdla48ut19g57mb9vdeh6uf2rvc9je6/1652662275000/01098719071037536088/*/1xUB9_bMs-PYnRbO7eedXFh0qS6P4bA-X?e=download [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2022-05-16 00:51:30--  https://doc-14-3s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/ksdla48ut19g57mb9vdeh6uf2rvc9je6/1652662275000/01098719071037536088/*/1xUB9_bMs-PYnRbO7eedXFh0qS6P4bA-X?e=download\n",
      "Resolving doc-14-3s-docs.googleusercontent.com (doc-14-3s-docs.googleusercontent.com)... 142.250.159.132, 2607:f8b0:4001:c58::84\n",
      "Connecting to doc-14-3s-docs.googleusercontent.com (doc-14-3s-docs.googleusercontent.com)|142.250.159.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2327012 (2.2M) [application/zip]\n",
      "Saving to: ‘dataset_assignment.zip’\n",
      "\n",
      "dataset_assignment. 100%[===================>]   2.22M  --.-KB/s    in 0.02s   \n",
      "\n",
      "2022-05-16 00:51:30 (118 MB/s) - ‘dataset_assignment.zip’ saved [2327012/2327012]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://drive.google.com/uc?export=download&id=1xUB9_bMs-PYnRbO7eedXFh0qS6P4bA-X\" -O dataset_assignment.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-p0omTr1tGkQ"
   },
   "source": [
    "Run the following command to unzip the zipped file. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2mM9Vq59rUvI",
    "outputId": "a880df6a-aacd-4aec-cbeb-08697ce83940"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  dataset_assignment.zip\n",
      "  inflating: dataset_assignment/stats  \n",
      "  inflating: dataset_assignment/test_set_sequences  \n",
      "  inflating: dataset_assignment/train_set_sequences  \n",
      "  inflating: dataset_assignment/val_set_sequences  \n"
     ]
    }
   ],
   "source": [
    "!unzip dataset_assignment.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LP5CafIssd1I"
   },
   "source": [
    "The unzipped folder contains the below-mentioned  files in it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JNll-jGgs_FX",
    "outputId": "8520f5f7-70c9-4ccc-ef3f-7f31a844d721"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat_rec_test.npy  LSTMRECModel.h5  test_set_sequences\tval_set_sequences\n",
      "cat_rec_val.npy   stats\t\t   train_set_sequences\n"
     ]
    }
   ],
   "source": [
    "!ls dataset_assignment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "QGVjtNL7Wlig",
    "outputId": "3402cb9d-96de-43ac-a500-5afe376f648b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "p94UkRa-uDjC",
    "outputId": "63f6975d-ba02-4768-c565-d730ed1ac4eb"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"\\n# train_set_sequences: File containing data of 5040 users and the movies they rated.\\n# val_set_sequences  : File containing data of 500  users and the movies they rated.\\n# test_set_sequences : File containing data of 500  users and the movies they rated.\\n\\n# For each of the 3 files, data for each user is present in different lines, which follows the following format-\\n# user_id movie_id1 rating1 movie_id2 rating2 …\\n\\nwith open('dataset_assignment/train_set_sequences') as f:\\n   # do something\\n\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To read the training data from the unzipped file-\n",
    "\n",
    "'''\n",
    "# train_set_sequences: File containing data of 5040 users and the movies they rated.\n",
    "# val_set_sequences  : File containing data of 500  users and the movies they rated.\n",
    "# test_set_sequences : File containing data of 500  users and the movies they rated.\n",
    "\n",
    "# For each of the 3 files, data for each user is present in different lines, which follows the following format-\n",
    "# user_id movie_id1 rating1 movie_id2 rating2 …\n",
    "\n",
    "with open('dataset_assignment/train_set_sequences') as f:\n",
    "   # do something\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "yd8qb4GGQq_w"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from keras.utils.np_utils import to_categorical   \n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "def read_data(file_name):\n",
    "  \"\"\"\n",
    "  This functions reads different data and saves them in dictionary\n",
    "  :param :str\n",
    "  :return :dict[list] --> dict_seq \n",
    "  :return :dict[list] --> dict_rating \n",
    "  \"\"\"\n",
    "  with open('dataset_assignment/'+file_name) as f:\n",
    "    x = f.readlines()\n",
    "  dict_seq = {}\n",
    "  dict_rating = {}\n",
    "  for i in x:\n",
    "    list_ = list(map(int, i.split()))\n",
    "    dict_seq[list_[0]] = list_[1::2]\n",
    "    dict_rating[list_[0]] = list_[2::2]\n",
    "  return(dict_seq, dict_rating)\n",
    "\n",
    "test = 'test_set_sequences'\n",
    "train = 'train_set_sequences'\n",
    "val = 'val_set_sequences'\n",
    "seq_train, rat_train = read_data(train)\n",
    "seq_test, rat_test = read_data(test)\n",
    "seq_val, rat_val = read_data(val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "skzItlH5W5g_"
   },
   "outputs": [],
   "source": [
    "# [1,3,4,567,7,4,2,24,6] --> length 9\n",
    "# [0,1,2,  3,4,5,6, 7,8] --> length 9\n",
    "# output = 1\n",
    "# now for each sequence in user we will create sub sequence\n",
    "def creat_seqsamples_upd(dict_seq, min_window = 5, num_sampl = 5):\n",
    "  \"\"\"\n",
    "  This function creates a sample of sequences on the basis of list given\n",
    "  :param :dict_user : dict --> key of the seq representing user and value representing seq\n",
    "  :return : list[int], list[list], list[list[int]] -->user, input_seq, recomendation\n",
    "  \"\"\"\n",
    "  input1 = [] # represents user seq\n",
    "  output1 = [] # represents input seq\n",
    "  output2 = [] # represents y as output or recommendation\n",
    "  for user in dict_seq:\n",
    "    lis = dict_seq[user]\n",
    "    seq_l = len(lis)\n",
    "    for j in range(num_sampl):\n",
    "      indic = random.randint(min_window,seq_l-2)\n",
    "      lis_x, lis_y = lis[0: indic+1], lis[indic+1:indic+2]\n",
    "      output1.append(lis_x)\n",
    "      output2.append(lis_y)\n",
    "      input1.append(user)\n",
    "  return(input1, output1, output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "CWN45JQ4imHb"
   },
   "outputs": [],
   "source": [
    "usr_train, sam_seq_train, rec_train = creat_seqsamples_upd(seq_train)\n",
    "usr_val, sam_seq_val, rec_val = creat_seqsamples_upd(seq_val)\n",
    "usr_test, sam_seq_test, rec_test = creat_seqsamples_upd(seq_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "NPzlJZrqWyfF"
   },
   "outputs": [],
   "source": [
    "def creat_num_data_upd(seq, rec, num_users=6040, num_items=3706, max_seq_len=10):\n",
    "  \"\"\"\n",
    "  This funtion takes in user , sequence and recommendation in list form and returns\n",
    "  corresponding ecoded outputs to be used by Rnn/Lstm achitecture Model\n",
    "  :param :user :list[int] --> user_list\n",
    "  :param :seq :list[list] --> seq_list\n",
    "  :param :rec :list[list] --> rec_list\n",
    "  :param :num_users :int --> num of distinct users\n",
    "  :param :num_items :int --> num of items\n",
    "  :param :max_seq_len :int --> maximum sequence length to be feeded in model\n",
    "  :return :array --> categorical_label_x denoting sequence array\n",
    "  :return :array --> categorical_label_y denoting label array [next prediction]\n",
    "  \"\"\"\n",
    "  pad_seq = sequence.pad_sequences(seq, maxlen= max_seq_len)\n",
    "  categorical_label_x = to_categorical(pad_seq, num_classes=num_items)\n",
    "  categorical_label_y = to_categorical(rec, num_classes=num_items)\n",
    "  return(categorical_label_x, categorical_label_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "QWtHbwVUORQZ"
   },
   "outputs": [],
   "source": [
    "def batch_cat_upd(sam_seq_train, rec_train, step = 500):\n",
    "  \"\"\"\n",
    "  This function tranforms data in batches so to have less memory issue\"\n",
    "  :param :list[list] --> sam_seq_train\n",
    "  :param :list[list] --> rec_train\n",
    "  :param :int --> step denoting batch size\n",
    "  :return :array --> seq array from different batches combined\n",
    "  :return :array --> label array from different batches combined\n",
    "  \"\"\"\n",
    "  l = len(sam_seq_train)\n",
    "  ite = int(np.ceil(l/step))\n",
    "  seq, rec = [], []\n",
    "  for i in range(0, ite):\n",
    "    print(i)\n",
    "    start = i*step\n",
    "    end = start + step\n",
    "    if i == ite:\n",
    "      end = l\n",
    "    cat_seq_train, cat_rec_train = creat_num_data_upd(sam_seq_train[start:end], rec_train[start:end])\n",
    "    seq.append(cat_seq_train)\n",
    "    rec.append(cat_rec_train)\n",
    "  seqf = np.concatenate(seq, axis=0)\n",
    "  recf = np.concatenate(rec, axis=0)\n",
    "  return(seqf, recf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Be1fAGnsYuNq"
   },
   "outputs": [],
   "source": [
    "#cat_seq_train, cat_rec_train = batch_cat_upd(sam_seq_train, rec_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t9DQqkG1WTpp"
   },
   "outputs": [],
   "source": [
    "cat_seq_test, cat_rec_test = batch_cat_upd(sam_seq_test, rec_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9JkHDB9tWT_I"
   },
   "outputs": [],
   "source": [
    "cat_seq_val, cat_rec_val = batch_cat_upd(sam_seq_val, rec_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iz7WAE4rUnG0"
   },
   "outputs": [],
   "source": [
    "\n",
    "#np.save(\"dataset_assignment/cat_usr_train.npy\", cat_usr_train)\n",
    "np.save(\"dataset_assignment/cat_seq_train.npy\", cat_seq_train)\n",
    "np.save(\"dataset_assignment/cat_rec_train.npy\", cat_rec_train)\n",
    "\n",
    "#np.save(\"dataset_assignment/cat_usr_test.npy\", cat_usr_test)\n",
    "np.save(\"dataset_assignment/cat_seq_test.npy\", cat_seq_test)\n",
    "np.save(\"dataset_assignment/cat_rec_test.npy\", cat_rec_test)\n",
    "\n",
    "#np.save(\"dataset_assignment/cat_usr_val.npy\", cat_usr_val)\n",
    "np.save(\"dataset_assignment/cat_seq_val.npy\", cat_seq_val)\n",
    "np.save(\"dataset_assignment/cat_rec_val.npy\", cat_rec_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C8DwHrQLW8Am"
   },
   "outputs": [],
   "source": [
    "# load array\n",
    "#cat_usr_train = np.load(\"dataset_assignment/cat_usr_train.npy\")\n",
    "cat_seq_train = np.load(\"dataset_assignment/cat_seq_train.npy\")\n",
    "cat_rec_train = np.load(\"dataset_assignment/cat_rec_train.npy\")\n",
    "#cat_usr_test = np.load(\"dataset_assignment/cat_usr_test.npy\")\n",
    "cat_seq_test = np.load(\"dataset_assignment/cat_seq_test.npy\")\n",
    "cat_rec_test = np.load(\"dataset_assignment/cat_rec_test.npy\")\n",
    "#cat_usr_val = np.load(\"dataset_assignment/cat_usr_val.npy\")\n",
    "cat_seq_val = np.load(\"dataset_assignment/cat_seq_val.npy\")\n",
    "cat_rec_val = np.load(\"dataset_assignment/cat_rec_val.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zR7vlGwx4EeK"
   },
   "outputs": [],
   "source": [
    "#cat_usr_train, cat_seq_train, cat_rec_train = creat_num_data(usr_train[0:1000], sam_seq_train[0:1000], rec_train[0:1000]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWbzqGLJrqPY"
   },
   "source": [
    "# Training Step\n",
    "\n",
    "Create and train a model that matches the SPS metric on the test set, that is mentioned in the paper. Also, the final model (or just model weights) should be saved so that it could be used for evaluating the metric later. The saved model (or model weights) should be named ‘best_model’ followed by a suitable extension, depending on the Deep Learning framework in use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G8dCcowxUS_w"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(cat_seq_train.shape[1], cat_seq_train.shape[2]), return_sequences=False))\n",
    "model.add(Dense(cat_rec_train.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yZIHFuIGOYY8",
    "outputId": "50fb6c5f-86f8-4478-da55-e8fb08fef1d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "504/504 [==============================] - 12s 22ms/step - loss: 7.0807\n",
      "Epoch 2/25\n",
      "504/504 [==============================] - 12s 23ms/step - loss: 6.9104\n",
      "Epoch 3/25\n",
      "504/504 [==============================] - 11s 23ms/step - loss: 6.6674\n",
      "Epoch 4/25\n",
      "504/504 [==============================] - 11s 22ms/step - loss: 6.4088\n",
      "Epoch 5/25\n",
      "504/504 [==============================] - 11s 22ms/step - loss: 6.1550\n",
      "Epoch 6/25\n",
      "504/504 [==============================] - 11s 22ms/step - loss: 5.9151\n",
      "Epoch 7/25\n",
      "504/504 [==============================] - 11s 22ms/step - loss: 5.6923\n",
      "Epoch 8/25\n",
      "504/504 [==============================] - 11s 22ms/step - loss: 5.4817\n",
      "Epoch 9/25\n",
      "504/504 [==============================] - 11s 22ms/step - loss: 5.2776\n",
      "Epoch 10/25\n",
      "504/504 [==============================] - 11s 22ms/step - loss: 5.0886\n",
      "Epoch 11/25\n",
      "504/504 [==============================] - 11s 23ms/step - loss: 4.9031\n",
      "Epoch 12/25\n",
      "504/504 [==============================] - 11s 21ms/step - loss: 4.7221\n",
      "Epoch 13/25\n",
      "504/504 [==============================] - 11s 22ms/step - loss: 4.5480\n",
      "Epoch 14/25\n",
      "504/504 [==============================] - 11s 22ms/step - loss: 4.3813\n",
      "Epoch 15/25\n",
      "504/504 [==============================] - 11s 22ms/step - loss: 4.2184\n",
      "Epoch 16/25\n",
      "504/504 [==============================] - 11s 22ms/step - loss: 4.0547\n",
      "Epoch 17/25\n",
      "504/504 [==============================] - 11s 22ms/step - loss: 3.9039\n",
      "Epoch 18/25\n",
      "504/504 [==============================] - 11s 22ms/step - loss: 3.7531\n",
      "Epoch 19/25\n",
      "504/504 [==============================] - 11s 23ms/step - loss: 3.6025\n",
      "Epoch 20/25\n",
      "504/504 [==============================] - 11s 23ms/step - loss: 3.4641\n",
      "Epoch 21/25\n",
      "504/504 [==============================] - 12s 24ms/step - loss: 3.3321\n",
      "Epoch 22/25\n",
      "504/504 [==============================] - 11s 22ms/step - loss: 3.2046\n",
      "Epoch 23/25\n",
      "504/504 [==============================] - 12s 24ms/step - loss: 3.0789\n",
      "Epoch 24/25\n",
      "504/504 [==============================] - 12s 24ms/step - loss: 2.9608\n",
      "Epoch 25/25\n",
      "504/504 [==============================] - 12s 23ms/step - loss: 2.8511\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7c10253310>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(cat_seq_train, cat_rec_train, epochs=25, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ewb1CPF2rUxO",
    "outputId": "662e7f4c-ed60-49fb-bd3f-b2180db15326"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 50)                751400    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3706)              189006    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 940,406\n",
      "Trainable params: 940,406\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mqVVUUW37Df9",
    "outputId": "4c3a8ca2-9be2-4d40-c236-14184ced4d12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved!\n"
     ]
    }
   ],
   "source": [
    "model.save('dataset_assignment/best_model.h5')\n",
    "print('Model Saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tPQyVv0Tr1t7"
   },
   "source": [
    "# Load model (or model weights)\n",
    "\n",
    "Load the saved model (or model weights). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iqfMDxpBoM6b",
    "outputId": "0f87b837-5d0a-4b72-98b3-1ed650c32ecc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 50)                751400    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3706)              189006    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 940,406\n",
      "Trainable params: 940,406\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "savedModel=load_model('dataset_assignment/best_model.h5')\n",
    "savedModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "zsVjfMg5yM7u"
   },
   "outputs": [],
   "source": [
    "val_pred_prob = savedModel.predict(cat_seq_val) # propabilites of different items predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "cvx60faG-8T5"
   },
   "outputs": [],
   "source": [
    "def trans_out(pred, no_of_rec = 10):\n",
    "  \"\"\"\n",
    "  This function transforms the predicted probabilites array into sereies of output vectors with 1s at\n",
    "  the item position for best n receommended items\n",
    "  :param  :array --> pred array consisting of probability vector of item vector for diff user\n",
    "  :param :int --> no_of_rec denoting no of recommendation to be taken from model\n",
    "  :return :array -->  array consisting of vectors for different users with 1s at diff indices in a row denoting items present in recommendation \n",
    "  \"\"\"\n",
    "  if no_of_rec == 1:\n",
    "    pred1 = np.argmax(pred, axis=1)\n",
    "    predf = to_categorical(pred1, num_classes=pred.shape[1])\n",
    "  else:\n",
    "    pred1 = np.argsort(pred, axis=1)\n",
    "    pred2 = np.flip(pred1, axis=1)[:,:no_of_rec]\n",
    "    pred3 = to_categorical(pred2, num_classes=pred.shape[1])\n",
    "    predf = np.sum(pred3, axis = 1)\n",
    "  return(predf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "yP4BqcFDBEc4"
   },
   "outputs": [],
   "source": [
    "val_pred_rec1 = trans_out(val_pred_prob, 1)\n",
    "val_pred_rec3 = trans_out(val_pred_prob, 3)\n",
    "val_pred_rec5 = trans_out(val_pred_prob, 5)\n",
    "val_pred_rec10 = trans_out(val_pred_prob, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "X9Tpt3HqBXDv"
   },
   "outputs": [],
   "source": [
    "def cal_sps(pred, y, sps_only=True):\n",
    "  \"\"\"\n",
    "  This function compared predicted array with ground truth for calculation of sps metric\n",
    "  dot product of pred with transpose of Y will result diagonal element of matrix to \n",
    "  be dot product of each item vectors for different user. summing of diagonal elemnt and avg over it no \n",
    "  of users result into sps per user or avg sps.\n",
    "  \"\"\"\n",
    "  mul = np.dot(pred, y.T)\n",
    "  bin_vector = np.diagonal(mul).reshape(len(y),1)\n",
    "  sps = bin_vector.sum(axis=0)[0]*100/len(y)\n",
    "  if sps_only == True:\n",
    "    return(sps)\n",
    "  return(bin_vector, sps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G-00tgBOHs3L",
    "outputId": "eaa5d689-cf0e-4fd5-9512-f516051e42f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sps @ 1 recommendation : 1.36%\n",
      "sps @ 3 recommendation : 2.88%\n",
      "sps @ 5 recommendation : 4.76%\n",
      "sps @ 10 recommendation : 8.56%\n"
     ]
    }
   ],
   "source": [
    "print(\"sps @ 1 recommendation : {}%\".format(cal_sps(val_pred_rec1, cat_rec_val)))\n",
    "print(\"sps @ 3 recommendation : {}%\".format(cal_sps(val_pred_rec3, cat_rec_val)))\n",
    "print(\"sps @ 5 recommendation : {}%\".format(cal_sps(val_pred_rec5, cat_rec_val)))\n",
    "print(\"sps @ 10 recommendation : {}%\".format(cal_sps(val_pred_rec10, cat_rec_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IgAvYV8Cr-oL"
   },
   "source": [
    "# Prediction Step\n",
    "\n",
    "Run the loaded model and evaluate it on the test data, printing the SPS metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "fovOgIQlrU9A"
   },
   "outputs": [],
   "source": [
    "test_pred_prob  = savedModel.predict(cat_seq_test)\n",
    "test_pred_rec1  = trans_out(test_pred_prob, 1)\n",
    "test_pred_rec3  = trans_out(test_pred_prob, 3)\n",
    "test_pred_rec5  = trans_out(test_pred_prob, 5)\n",
    "test_pred_rec10 = trans_out(test_pred_prob, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B5n_SkHPKggj",
    "outputId": "fa0274c2-3ebf-4a52-f902-bad12e9a167f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sps @ 1 recommendation : 1.0%\n",
      "sps @ 3 recommendation : 3.2%\n",
      "sps @ 5 recommendation : 4.92%\n",
      "sps @ 10 recommendation : 8.48%\n"
     ]
    }
   ],
   "source": [
    "print(\"sps @ 1 recommendation : {}%\".format(cal_sps(test_pred_rec1, cat_rec_test)))\n",
    "print(\"sps @ 3 recommendation : {}%\".format(cal_sps(test_pred_rec3, cat_rec_test)))\n",
    "print(\"sps @ 5 recommendation : {}%\".format(cal_sps(test_pred_rec5, cat_rec_test)))\n",
    "print(\"sps @ 10 recommendation : {}%\".format(cal_sps(test_pred_rec10, cat_rec_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7NydXNXKiXf"
   },
   "source": [
    "# QnA Section:\n",
    "Please answer the following questions based on your understanding of your paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOsBp4268jml"
   },
   "source": [
    "### Question 1:\n",
    "\n",
    "You have a set of 3 sequences:\n",
    "```\n",
    "seq1 = ['item1', 'item5', 'item3', 'item2', 'item4']\n",
    "seq2 = ['item10', 'item5', 'item4', 'item8', 'item2', 'item1', 'item3']\n",
    "seq3 = ['item8', 'item2', 'item5', 'item3', 'item2', 'item10']\n",
    "```\n",
    "\n",
    "We take the first 3 items from all these sequences, and feed them in a RNN model.\n",
    "The model generates outputs for 3 timesteps for all the 3 input sequences.\n",
    "Following are the model outputs, where out1 corresponds to the output for input sequence seq1, and so on-\n",
    "```\n",
    "out1 = ['item2', 'item4', 'item1']   \n",
    "out2 = ['item2', 'item10', 'item1']   \n",
    "out3 = ['item2', 'item3', 'item5']   \n",
    "```\n",
    "\n",
    "What will be the sps@3 of this RNN model, for the given set of input sequences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dLv0u17n_CaS"
   },
   "outputs": [],
   "source": [
    "# Answer 1:  \n",
    "\"\"\"\n",
    "seq 1 --> 1/3 item 2 present in rec\n",
    "seq 2 --> 0/3 item 8 not present in rec\n",
    "seq 3 --> 1/3 item 3 present in rec\n",
    "\n",
    "sps@3 == 66.67%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xrxxewi9zoT"
   },
   "source": [
    "### Question 2:\n",
    "\n",
    "Which do you think is the most significant metric for a recommendation system: sps or recall?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "81Z9xg1x_C9B"
   },
   "outputs": [],
   "source": [
    "# Answer 2: \n",
    "\"\"\"\n",
    "Recall [NDCG is even better] is better for recommendation system. \n",
    "sps is good for short term sequence prediction.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQ2CQrYt83U5"
   },
   "source": [
    "### Question 3:\n",
    "\n",
    "How does the paper propose to measure the short term / long term profiling of a recommender system?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eSX6XvTC_Dgd"
   },
   "outputs": [],
   "source": [
    "# Answer 3: \n",
    "\"\"\"\n",
    "By measuring average of avg-r@N over all users of test data , where avg-r@N is less for small N giving profound usefullness \n",
    "for short term prediction but increases linearly as N increases denoting quality of prediction reduces as Long term prediction \n",
    "is affected by change in taste etc.\n",
    "Note : lesser is the avg-r@N , lesser is avg rank over N means each prediction has less rank which means closer to actual truth, meaning \n",
    "better prediction/recommendation\n",
    "\n",
    "lesser the avg-r@N at higher N shows better generalized rec model\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jc7QQ5gn8K4Y"
   },
   "source": [
    "### Question 4: \n",
    "Suppose you have a user U who has watched 2*n movies, and you have trained RNN based recommender with first n movies. Now, you have a number N <= n, denoting the number of next items in the user sequence taken to do short-term/long-term profiling. \n",
    "As per the paper, what impact does increasing N have on this profiling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LPi84Ifd8Tnh"
   },
   "outputs": [],
   "source": [
    "# Answer 4:\n",
    "\"\"\"\n",
    "As N increases avg-r@N increases smoothly/linearly for various model denoting as no of Next Sequence increases, the avg rank also increases leading to \n",
    "far less viable recommendation denoting the preference of user changes as time varies\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4paTeZVW9vHM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tAb77yZ9fzMG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pafL7Li0jyXW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Long and Short-Term Recommendations with Recurrent Neural Networks.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
